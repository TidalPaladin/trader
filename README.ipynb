{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzaxUZZLYJYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5dJsH9JYLD8",
        "colab_type": "text"
      },
      "source": [
        "# TraderNet\n",
        "\n",
        "TraderNet was created as a final project for CS6350 - big data. It\n",
        "attempts to predict future stock prices either through classification\n",
        "or regression. It incorporates distributed computing technologies like Spark to\n",
        "preprocess a large number of historical stock price / volume records\n",
        "into training examples that are then fed to a deep neural network\n",
        "architecture implemented in Tensorflow. The aim of the network is to\n",
        "aid in prioritizing technical analysis of stocks and serve as a basis\n",
        "for future reinforcement learning models that will suggest trades\n",
        "automatically.\n",
        "\n",
        "## Preprocessing\n",
        "\n",
        "### Dataset\n",
        "\n",
        "The input dataset (taken from\n",
        "[this](https://www.kaggle.com/qks1lver/amex-nyse-nasdaq-stock-histories)\n",
        "Kaggle challenge) consists of approximately $8000$ CSV files, each of which\n",
        "contains historical price records for a stock symbol from 1970\n",
        "to 2018. Each record consists of the following columns:\n",
        "\n",
        "```\n",
        "date, high, low, open, close, volume, adjclose\n",
        "```\n",
        "\n",
        "The high, low, open, and close prices are historical prices as\n",
        "recorded by the market on that day. The `adjclose` column provides an\n",
        "adjusted close price that accounts for events like splits, which\n",
        "retroactively affect historical price data. For example, if a company doubles its number of existing shares, the\n",
        "price per share will be reduced by half. The adjusted close price\n",
        "accounts for these changes, providing a shared scale on which to\n",
        "measure closing price over the history of the stock.\n",
        "\n",
        "### Filtering\n",
        "\n",
        "Several filtering techniques can be applied to the input dataset in\n",
        "order to produce higher quality training examples. One such technique\n",
        "is the removal records older than a parameterized year.\n",
        "Since the dataset contains records dating back to 1970, the\n",
        "ability to constrain to more recent records may prove useful in\n",
        "capturing market behavior under modern trading styles.\n",
        "\n",
        "Another filtering technique is to remove penny stocks, which tend to exhibit greater\n",
        "volatility due to their low price per share. This is accomplished\n",
        "through the use of a parameterized closing price threshold. If the\n",
        "average closing price of a stock over the constrained date window is\n",
        "below the parameterized threshold, that stock will be excluded\n",
        "entirely from the training example set.\n",
        "\n",
        "### Positional Encoding\n",
        "\n",
        "A positional encoding strategy was used to pass date information from\n",
        "each record to the neural network in a numerically stable form. The underlying assumption\n",
        "is that stocks may behave differently at different times of year (e.g.\n",
        "during quarterly earnings reports, end of year, etc.), and thus this\n",
        "information could prove useful to the network. The following function\n",
        "was used to calculate a positional encoding based on the day of year:\n",
        "\n",
        "$$\n",
        "f(x) = \\sin\\left(\\frac{\\pi x}{365}\\right)\n",
        "$$\n",
        "\n",
        "This function transforms a day of year on the interval $[1, 365]$ into a real number on a\n",
        "continuous interval from $(0, 1]$. Under this transformation, day\n",
        "$365$ and day $1$ will have similar values, capturing the idea that\n",
        "December 31st and January 1st are close to each other. It is worth\n",
        "noting that each point under this transformation will be produced by\n",
        "two points in the original \"day of year\" space. This is not ideal, but\n",
        "the network should be able to compensate based on whether or not the\n",
        "point lies on an increasing or decreasing region in the encoded space.\n",
        "\n",
        "### Price Adjustment\n",
        "\n",
        "The close and adjusted close price given in the original dataset were used to\n",
        "rescale the other original price metrics (high, low, open). This was\n",
        "accomplished by apply the following transformation to each unscaled\n",
        "price in the record:\n",
        "\n",
        "$$\n",
        "\\text{scaled} = \\text{unscaled} \\cdot\n",
        "\\frac{\\text{adjclose}}{\\text{close}}\n",
        "$$\n",
        "\n",
        "After this point the unscaled prices were discarded. This places all\n",
        "prices on a shared scale that is invariant to stock splits and other\n",
        "price-altering events, improving continuity.\n",
        "\n",
        "### Future Price Calculation\n",
        "\n",
        "A future percent change in closing price was calculated for each\n",
        "record by looking $x$ days into the future and calculating a pointwise\n",
        "or aggregate measure of closing price over those $x$ days. One such\n",
        "strategy is to calculate a pointwise percent change in\n",
        "price as follows, where $i$ indicates an index of the record to be\n",
        "labeled:\n",
        "\n",
        "$$\n",
        "\\Delta = \\frac{P_{i+x} - P_{i}}{P_i} * 100\n",
        "$$\n",
        "\n",
        "Another strategy is to calculate an aggregate of some price metric\n",
        "over $x$ days in the future and calculate percent change from the present\n",
        "price to that aggregate price. For example, one could calculate an average closing\n",
        "price over the following $x$ days and then calculate percent change\n",
        "in closing price from the present to that average.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\t&\\Delta = \\frac{a - P_{i}}{P_i} * 100\n",
        "\t&\\text{where}&\n",
        "\t&a = \\frac{1}{x}\\sum_{j=i+1}^{i+x} P_j\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The decision to use a pointwise or aggregate basis for percent change\n",
        "calculation is dependent on the choice of window size among other\n",
        "things. The choice of window size is primarily determined by the style\n",
        "of trading to be used, along with the interval between records (if\n",
        "this approach were to be applied to by the minute data for day\n",
        "trading). For experimentation, percent change in future price was\n",
        "calculated relative to the average closing price over a future $3$ day\n",
        "window.\n",
        "\n",
        "### Spark ML Pipeline\n",
        "\n",
        "A parameterized Spark ML pipeline is used to extract feature vectors and produce\n",
        "discretized labels for each record. The pipeline consists of the\n",
        "following stages:\n",
        "\n",
        "1. `VectorAssembler` - Collects feature columns into a single vector\n",
        "\tfor processing\n",
        "\n",
        "2. Normalizer - A strategy to normalize each feature to a shared\n",
        "\tdomain. Can be one of the following, or excluded entirely:\n",
        "\t* `StandardScaler` - Rescales each feature to zero mean unit variance\n",
        "\t* `MaxAbsScaler` - Rescales each feature to the range $[0, 1]$,\n",
        "\t\t**preserving sparsity**\n",
        "\n",
        "3. Labeler - A strategy to assign examples a discretized label based\n",
        "\t on future percent change. Can be one of the following, or excluded entirely:\n",
        "\t* `Bucketizer` - Label based on fixed sized buckets of percent\n",
        "\t\tchange\n",
        "\t* `QuantileDiscretizer` - Label based on variable sized buckets such\n",
        "\t\tthat each each class contains an equal number of examples\n",
        "\n",
        "The each record in the output of this pipeline will have a vector of\n",
        "(normalized) features for that day, a future percent change value, and\n",
        "an integer label if requested. By retaining both the future percent\n",
        "change and a discretized label, each record is suitable for use with\n",
        "classification or regression networks.\n",
        "\n",
        "It is important to consider the distribution of examples among the\n",
        "possible classes when using `Bucketizer` as a labeling strategy.\n",
        "It was observed that percent change tends to concentrate near zero,\n",
        "meaning that the bucket that overlaps with zero will have a\n",
        "potentially large number of examples. This is likely to become more of\n",
        "an issue with larger future window sizes, as price will have more time\n",
        "to regress to the mean.\n",
        "\n",
        "Conversely, when using\n",
        "`QuantileDiscretizer` a uniform distribution of examples over each\n",
        "class will be created by compressing or expanding bucket ranges such\n",
        "that the $n$'th bucket contains the top $1/n$'th percent change values.\n",
        "This will manifest as buckets that span only a few percentage points of\n",
        "percent change, taxing the network's ability to distinguish these\n",
        "classes. However, it does\n",
        "eliminate concerns that the network will exhibit a bias towards zero\n",
        "price movement predictions.\n",
        "\n",
        "During experimentation the `MaxAbsScaler` was used as a normalization\n",
        "strategy and the `Bucketizer` was used for a labeling strategy with\n",
        "a bucket parameter list of $-5,-2,2,5$. This produced five discretized\n",
        "classes in the resultant training set. Unequal bucket widths\n",
        "were important in creating a somewhat balanced distribution of\n",
        "examples among the labels while not forcing the network to learn\n",
        "unreasonably small differences between the classes.\n",
        "\n",
        "\n",
        "## Creating Training Examples\n",
        "\n",
        "Having extracted numerically stable features and labels from the\n",
        "original dataset, steps must now be taken to construct complete training\n",
        "examples. Taking inspiration from human styles of trading, the\n",
        "decision was made to create training examples by aggregating\n",
        "chronologically ordered feature vectors over a sliding historical window into a\n",
        "feature matrix.\n",
        "\n",
        "As a concrete example, consider training example $i$ with features\n",
        "$h,l,o,c,v,p$ and a window size of $x$. Example $i$ would then consist\n",
        "of features over the $x$ previous days for that stock as follows:\n",
        "\n",
        "$$\n",
        "\\text{example}_i = \\begin{bmatrix}\n",
        "\th_i & l_i & o_i & c_i & v_i & p_i \\\\\n",
        "\th_{i-1} & l_{i-1} & o_{i-1} & c_{i-1} & v_{i-1} & p_{i-1} \\\\\n",
        "\th_{i-2} & l_{i-2} & o_{i-2} & c_{i-2} & v_{i-2} & p_{i-2} \\\\\n",
        "\t\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "\th_{i-x} & l_{i-x} & o_{i-x} & c_{i-x} & v_{i-x} & p_{i-x} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The result of this process is an intuitively constructed training\n",
        "example, consisting of an ordered time series of features over the $x$\n",
        "previous days and a label indicating price movement in the near\n",
        "future. Implementation of this process is discussed in the following\n",
        "section.\n",
        "\n",
        "We can take further steps to improve the quality of the resultant\n",
        "training examples. Consider the case where the process described above\n",
        "is applied to two adjacent records, $i$ and $i+1$. The result will be\n",
        "two training examples with $x \\times 6$ feature matrices which have a\n",
        "substantial overlap. Specifically, we will produce feature matrices as\n",
        "follows:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\tfeatures_i \\in \\mathbb{R}^{x \\times 6} &=\n",
        "\t\\begin{bmatrix}\n",
        "\t\th_i & l_i & o_i & c_i & v_i & p_i \\\\\n",
        "\t\t\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "\t\th_{i-x} & l_{i-x} & o_{i-x} & c_{i-x} & v_{i-x} & p_{i-x} \\\\\n",
        "\t\\end{bmatrix}\n",
        "\t\\\\\n",
        "\tfeatures_{i+1} \\in \\mathbb{R}^{x \\times 6} &=\n",
        "\t\\begin{bmatrix}\n",
        "\t\th_{i+1} & l_{i+1} & o_{i+1} & c_{i+1} & v_{i+1} & p_{i+1} \\\\\n",
        "\t\t\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "\t\th_{i-x+1} & l_{i-x+1} & o_{i-x+1} & c_{i-x+1} & v_{i-x+1} & p_{i-x+1} \\\\\n",
        "\t\\end{bmatrix}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Applying a stride greater than $1$ to the historical window will\n",
        "reduce the amount of overlap between feature matrices, creating a\n",
        "more diverse training set. For experimentation a stride of $5$ days\n",
        "was chosen. This resulted in an adequate number of training examples\n",
        "and minimized the overlap in the $3$ day future percent change\n",
        "window.\n",
        "\n",
        "Generating the feature matrices as described above is a\n",
        "computationally expensive process, especially at small window\n",
        "strides and large window sizes. To put this in perspective, consider a\n",
        "training set produced from $1000$ stock symbols from $2008$ to $2018$\n",
        "with a window size of $128$ days and a window stride of $1$. Assume\n",
        "the stock only trades $200$ days out of the year.  The\n",
        "number of resultant examples is given by\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\tN &= 1000 * 200 * (2018 - 2008) \\\\\n",
        "\t&= 2,000,000\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Similarly, if we consider this process applied the entirety of the\n",
        "dataset without constraint\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\tN &= 8000 * 200 * (2018 - 1971) \\\\\n",
        "\t&= 75,200,000\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We will have a $128 \\times 6$ feature matrix per row.\n",
        "If we assume that each feature and the percent change is stored as a\n",
        "$32$ bit float and the label is stored as an $8$ bit integer, we can\n",
        "calculated the expected size of the resultant training set in\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\tfeatures &= N \\left(128 * 6 * 4\\right) \\\\\n",
        "\tlabels &= N \\left(4 + 1\\right) \\\\\n",
        "\tS &= labels + features \\\\\n",
        "\t&\\approx 230 GB\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "## Interfacing with Tensorflow\n",
        "\n",
        "The above illustrates the need for careful consideration on how\n",
        "Tensorflow should interface with Spark in order to handle the data\n",
        "volume. Several strategies were considered:\n",
        "\n",
        "### ~~Connect Tensorflow's computational graph to Spark~~\n",
        "Some libraries do exist which promise to connect Tensorflow and Spark at\n",
        "the graph level, allowing for training of a Tensorflow model in\n",
        "Spark. This approach was ultimately not used, as it would require\n",
        "sacrificing many of Tensorflow's high level features and presented\n",
        "compatibility issues with Tensorflow 2.0. It would however avoid the\n",
        "need to write the entire training set to disk prior to training and\n",
        "would expedite the process of iteratively refining preprocessing\n",
        "techniques based on model performance.\n",
        "\n",
        "### Write training examples to TFRecords\n",
        "Tensorflow uses `TFRecords` as a format for storing training examples in a\n",
        "serialized form. A dataset can be split in to multiple TFRecord\n",
        "files, enabling distributed training. Through the use of [spark-tensorflow-connector](https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-connector)\n",
        "it is possible to generate TFRecords from a Spark dataframe, where the\n",
        "number of TFRecord files is determined by the partitioning of the\n",
        "dataframe. Initial efforts aimed to partition the records by stock\n",
        "symbol, order them by date, and write each partition to a separate\n",
        "TFRecord file. Under this strategy, Tensorflow would be responsible\n",
        "for constructing training examples using the feature matrix generation\n",
        "procedure described earlier.\n",
        "\n",
        "By allowing Tensorflow to handle feature generation the size of the\n",
        "TFRecord dataset will be reduced by a factor of the historical window\n",
        "size. Unfortunately, this technique has the following requirements:\n",
        "\n",
        "1. Strict partitioning by symbol to separate TFRecord files\n",
        "2. Strict ordering by record date within TFRecord files\n",
        "3. A complicated process to interleave examples from each file\n",
        "\n",
        "According to\n",
        "[this](https://github.com/tensorflow/ecosystem/issues/119) issue,\n",
        "spark-tensorflow-connector does not currently respect the requested\n",
        "partitioning by column, making requirement 1 unsatisfiable. This\n",
        "necessitated the use of spark for feature matrix generation.\n",
        "\n",
        "The training examples written to TFRecords by spark are complete\n",
        "examples, containing a feature matrix, percent change value, and\n",
        "discretized label (if requested). Tensorflow is able to deserialize\n",
        "these examples with little additional processing.\n",
        "\n",
        "The following steps are performed in Tensorflow to finalize the\n",
        "dataset for training:\n",
        "\n",
        "1. Create `(feature, label)` tuples for each example where the label\n",
        "\t is chosen based on the current mode (regression or classification)\n",
        "2. Split the dataset into training and validation sets based on a\n",
        "\t parameterized validation set size.\n",
        "3. Batch training and validation sets into a parameterized batch size.\n",
        "\t A batch size of $128$ or $256$ was used in an effort to improve tiling\n",
        "\t efficiency when training on a GPU\n",
        "4. Final modifications like `repeat()` and `prefetch()`\n",
        "\n",
        "\n",
        "## Model\n",
        "\n",
        "TraderNet is based on a dominant object detection network I\n",
        "have been implementing\n",
        "[here](https://github.com/TidalPaladin/tiny-imagenet-demo). TraderNet\n",
        "reduces the original 2D convolutional encoder to a 1D residual\n",
        "encoder, with the addition of a multi-head attention layer at the head\n",
        "to better distinguish features based on their sequential relationship.\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "A summary of the model generated by Tensorflow is shown below. By\n",
        "default there are four levels of downsampling with $3, 3, 5, 3$\n",
        "bottleneck block repeats respectively. During experimentation, it was\n",
        "found that a $4, 6, 10, 4$ level repeat structure was more robust, and\n",
        "is shown below:\n",
        "\n",
        "```\n",
        "Model: \"trader_net\" - modified from model.summary()\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #\n",
        "=================================================================\n",
        "tail (Tail)                  (None, 128, 32)           672\n",
        "_________________________________________________________________\n",
        "4x bottleneck (Bottleneck)      (None, 128, 32)        696\n",
        "_________________________________________________________________\n",
        "downsample (Downsample)      (None, 64, 64)            8048\n",
        "_________________________________________________________________\n",
        "6x bottleneck (Bottleneck)   (None, 64, 64)            2416\n",
        "_________________________________________________________________\n",
        "downsample_1 (Downsample)    (None, 32, 128)           31456\n",
        "_________________________________________________________________\n",
        "10x bottleneck (Bottleneck)   (None, 32, 128)          8928\n",
        "_________________________________________________________________\n",
        "downsample_2 (Downsample)    (None, 16, 256)           124352\n",
        "_________________________________________________________________\n",
        "4x bottleneck (Bottleneck)   (None, 16, 256)           34240\n",
        "_________________________________________________________________\n",
        "downsample_3 (Downsample)    (None, 8, 512)            494464\n",
        "_________________________________________________________________\n",
        "attn (MultiHeadAttention)    (None, None, 512)         1051649\n",
        "_________________________________________________________________\n",
        "head (ClassificationHead)    (None, 5)                 2565\n",
        "=================================================================\n",
        "Total params: 1,932,726\n",
        "Trainable params: 1,922,902\n",
        "Non-trainable params: 9,824\n",
        "_________________________________________________________________\n",
        "```\n",
        "\n",
        "### Loss / Metrics\n",
        "\n",
        "When operating in classification mode, the model uses a sparse softmax\n",
        "cross-entropy loss. Sparse categorical accuracy is continuously\n",
        "tracked for the training set and is reported once per epoch for the\n",
        "validation set. Depending on the number of discretized classes it may\n",
        "be helpful to add a top k categorical accuracy in the case of closely\n",
        "related classes.\n",
        "\n",
        "In regression mode the model uses mean squared error as a loss\n",
        "function and is tracked with the same frequency as the classification\n",
        "pipeline. While regression functionality was retained in the network\n",
        "implementation, regression was not ultimately used for final\n",
        "experimentation.\n",
        "\n",
        "### Hyperparameters\n",
        "\n",
        "Key tunable hyperparameters include the following:\n",
        "\n",
        "1. **Dropout** - A dropout layer can be added after the attention\n",
        "\t layer to make the model more robust. This layer was excluded from\n",
        "\t the final project submission, as it would be impractical to train\n",
        "\t the model to convergence in the time alloted to complete the\n",
        "\t assignment.\n",
        "\n",
        "2. **Batch size** - Tune this as needed for your hardware. It is\n",
        "\t probably wise to use a power of two batch size in order to improve\n",
        "\t tiling efficiency for matrix matrix multiplication.\n",
        "\t A batch size of $256$ was typically used for experiments.\n",
        "\n",
        "3. **Learning rate** - Learning rate can be tuned directly, with\n",
        "\t values around $0.001$ tending to work well. A learning rate greater\n",
        "\t than $0.1$ often hindered the training process, likely due to the\n",
        "\t lack of a robust weight initialization strategy. Learning rate is\n",
        "\t reduced in a stepwise manner using a parameterized Tensorflow\n",
        "\t callback based on the change in loss between epochs.\n",
        "\n",
        "4. **Regularization** - Some experiments were conducted with\n",
        "\t regularization of the dense layers at the network head, but there\n",
        "\t appeared to be no discernible impact on model performance. This may\n",
        "\t be more significant when training on a small subset of the original\n",
        "\t dataset, such as a single stock or a narrow range of dates. Given\n",
        "\t the size of the training set, the use of regularization as a method\n",
        "\t of combating overfitting is likely unwarranted.\n",
        "\n",
        "### Monitoring\n",
        "\n",
        "Tensorboard callbacks are used to provide an interface for monitoring\n",
        "the training process and hyperparameter tuning. Use of Tensorboard is\n",
        "discussed in the usage section below. Model weights are checkpointed\n",
        "once per epoch and can be restored when entering a new training session.\n",
        "\n",
        "\n",
        "## Usage\n",
        "\n",
        "There are two separate usage procedures, one for the Scala JAR file\n",
        "that handles TFRecord production, and one for the Python Tensorflow\n",
        "training pipeline.\n",
        "\n",
        "### Spark\n",
        "\n",
        "The [spark](./spark) directory contains a `sbt` project that can be\n",
        "used to package a fat JAR file. The fat JAR file contains all\n",
        "dependencies (like spark-tensorflow-connector) but can become large.\n",
        "Build the fat JAR locally using `sbt assembly`.\n",
        "\n",
        "The program provides usage instructions via the `--help` flag as follows:\n",
        "\n",
        "```\n",
        "trader 1.0\n",
        "Usage: trader [options] <shell_glob>\n",
        "\n",
        "  -o, --out <path>         output file path\n",
        "  -n, --norm std, maxabs   Normalization with StandardScaler or MaxAbsScaler\n",
        "  -d, --date <year>        limit to records newer than <year>\n",
        "  -p, --past <int>         aggregate past <int> records into an example\n",
        "  --stride <int>           stride training example window by <int> days\n",
        "  -f, --future <int>       calculate percent change over <int> following days.\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t if >1, use averaging\n",
        "  --max-change <float>     drop examples with absolute percent change > <float>\n",
        "  -s, --shards <int>       split the TFRecord set into <int> shards\n",
        "  --quantize <int>         run QuantileDiscretizer to <int> buckets\n",
        "  --bucketize <float>,[float,...]\n",
        "                           run Bucketizer with the given buckets\n",
        "  --penny-stocks           if set, allow penny stocks. default false\n",
        "  --help                   prints this usage text\n",
        "Note: TFRecords will be written if output file path is specified\n",
        "\n",
        "Writing many TFRecords has a high RAM requirement.\n",
        "\n",
        "  <shell_glob>             input file shell glob pattern\n",
        "```\n",
        "\n",
        "If you run the program without specifying an output file path, no\n",
        "TFRecords will be produced, but dataframes will still be printed that\n",
        "describe the processed output. **Note* that the process of creating\n",
        "TFRecords can have significant CPU, memory, and disk requirements\n",
        "depending on the combination of flags given. The configuration used\n",
        "for experimentation\n",
        "would require approximately 40GB of memory and the resultant TFRecords\n",
        "would occupy 10G of disk space.\n",
        "\n",
        "The `<shell_glob>` argument should be path or wildcard containing path\n",
        "(e.g. `path/to/dir/*.csv`). This string will be fed directly to Spark's\n",
        "`DataFrameReader.csv()`.\n",
        "\n",
        "**For simple testing** the most effective strategy is to choose a\n",
        "shell glob pattern that will select only a small number of CSV files,\n",
        "e.g. `AAPL.csv` or `AAP*.csv`. Please see the [output](./output)\n",
        "directory for sample commands and output. It may be necessary to wrap\n",
        "your glob pattern as a string literal (e.g. `'*.csv'`) to prevent\n",
        "shell expansion to multiple filename arguments.\n",
        "\n",
        "### Tensorflow\n",
        "\n",
        "TraderNet is packaged as a Docker container to effectively manage\n",
        "dependencies on Tensorflow 2.0 and Tensorboard. A docker-compose file\n",
        "is provided to simplify the build process, and allows for selection of\n",
        "the Tensorflow base image tag (GPU or no GPU). Edit the docker-compose\n",
        "`upstream` build arg to pick the desired Tensorflow container tag. You can\n",
        "download Tensorflow 2.0 and run the model outside of Docker if\n",
        "desired.\n",
        "\n",
        "A `.env` file is provided that sets environment variables needed\n",
        "when starting the container with docker-compose. These variables\n",
        "include paths to the TFRecord source directory and log / checkpoint\n",
        "output directory:\n",
        "\n",
        "```\n",
        "SRC_DIR=/path/to/tfrecords\n",
        "ARTIFACT_DIR=/path/to/logs\n",
        "```\n",
        "\n",
        "Set these variables as desired or override them directly in the\n",
        "docker-compose file. When starting the container with docker-compose,\n",
        "these directories will be mounted as container volumes. The paths\n",
        "given to local directories must be absolute paths in order for Docker\n",
        "to mount the path as a volume.\n",
        "\n",
        "Next, start the container with\n",
        "\n",
        "```\n",
        "docker-compose up -d trader\n",
        "```\n",
        "\n",
        "You should be able to visit `localhost:6006` to access Tensorboard.\n",
        "Once the container is running, start the training pipeline using\n",
        "\n",
        "```\n",
        "docker exec -it trader python /app/train.py\n",
        "```\n",
        "\n",
        "For example,\n",
        "\n",
        "```\n",
        "docker exec -it trader python /app/train.py --classes 5 \\\n",
        "\t--levels 4,6,10,4 --speedrun\n",
        "```\n",
        "\n",
        "The `train.py` script accepts several command line flags that\n",
        "parameterize the training process and can be accessed using the\n",
        "`--helpfull` flag.\n",
        "\n",
        "```\n",
        "docker exec -it trader python /app/train.py --helpfull\n",
        "```\n",
        "\n",
        "Notable flags include:\n",
        "\n",
        "1. `--classes` - **Must** be set to the number of labels in the\n",
        "\t TFRecord dataset\n",
        "\n",
        "2. `--summary` - Prints the network summary and exits\n",
        "\n",
        "3. `--src, --dest` - Defaults to Docker volume mounts from docker\n",
        "\t compose\n",
        "\n",
        "4. `--mode` - Toggle from default `classification` mode to\n",
        "\t `regression` mode\n",
        "\n",
        "5. `--speedrun` - Runs a quick demo to show that the pipeline is\n",
        "\t operating correctly. Runs through very small epochs, printing model\n",
        "\t output for a small number of training examples.  This can be useful\n",
        "\t when combined with the `--resume` flag to evaluate a trained model\n",
        "\t given a saved model weights file.\n",
        "\n",
        "6. `--artifacts_dir` - Directory where Tensorboard logs, model\n",
        "\t checkpoints, etc will be written. Defaults to a Docker volume\n",
        "\n",
        "7. `--resume` - Filepath for a model weights file. The model will be\n",
        "\t initialized with these weights and will resume training.\n",
        "\n",
        "8. `--resume_last` - Attempts to automatically detect the last model\n",
        "\t checkpoint file and resume training from the checkpointed weights.\n",
        "\n",
        "\n",
        "## Results / Conclusions\n",
        "\n",
        "The model's classification performance was ultimately benchmarked using both\n",
        "categorical accuracy and top $2$ categorical accuracy. Due to the\n",
        "model's complexity, training to convergence was impractical in the\n",
        "timeframe alloted for the assignment, but the model still performed\n",
        "reasonably well. The plots below show sparse categorical and top k\n",
        "accuracy for several (interrupted) runs. Validation top $1$ categorical\n",
        "accuracy reached as high as $51\\%$, while top 2 categorical accuracy\n",
        "reached $74\\%$. The model took between five and ten minutes per epoch depending\n",
        "on choice of batch size, which each epoch containing on average\n",
        "$400,000$ training examples. Note that this training set was a\n",
        "fraction of the potential number of training examples that could be\n",
        "generated from the original dataset. By relaxing price and date\n",
        "constraints it is possible to generate a much larger number of\n",
        "examples if one has the resources to train on such a sample size.\n",
        "\n",
        "The graphs below were extracted from Tensorboard. Note that the broken\n",
        "lines in the plot below are the result of interrupted training sessions\n",
        "that were resumed using model checkpoints. Further work is needed to\n",
        "produce a clean and continuous plot of network performance as time\n",
        "becomes available.\n",
        "\n",
        "\n",
        "![Top 1 Accuracy vs Epoch](./epoch_sparse_categorical_accuracy.svg)\n",
        "\n",
        "\n",
        "![Top 2 Accuracy vs Epoch](./epoch_sparse_top_k_categorical_accuracy.svg)\n",
        "\n",
        "\n",
        "Though the accuracy reached by the model in the available training\n",
        "window is acceptable given the difficulty that humans have in\n",
        "predicting stock movement, this model would likely perform much better\n",
        "given more training time. Given that the number of potential\n",
        "training examples nears that of ImageNet (which may take month to\n",
        "train with state of the art models), I suspect that the few hours\n",
        "of training alloted to this model across various hyperparameter\n",
        "tuning runs was insufficient to demonstrate the model's full\n",
        "potential.\n",
        "\n",
        "A `.hdf5` file containing the trained model weights is provided\n",
        "in the root directory and can be used with the `--resume` and\n",
        "`--speedrun` flags to perform a rough evaluation as follows:\n",
        "\n",
        "Copy the model weights file into the running Docker container. It is\n",
        "important that the weights file be of the form `name_epoch.hdf5`, as\n",
        "the epoch will be read from the filename when resuming.\n",
        "\n",
        "```\n",
        "docker cp trader_27.hdf5 trader:/\n",
        "```\n",
        "\n",
        "Then conduct a speedrun using the saved model weights. The level\n",
        "parameterization must match what is given below in order for the\n",
        "weights to load correctly.\n",
        "\n",
        "```\n",
        "docker exec -it trader python /app/train.py \\\n",
        "\t--dry --classes 5 --levels 4,6,10,4 \\\n",
        "\t--resume=/trader_27.hdf5 --speedrun\n",
        "```\n",
        "\n",
        "## Future Work\n",
        "\n",
        "There are several areas where the preprocessing and training pipeline\n",
        "can be improved. These include:\n",
        "\n",
        "1. Adding steps to produce metrics like exponential moving average and\n",
        "\t add them to the\n",
        "\t Spark pipeline. This may allow the model to train faster by not\n",
        "\t forcing it to learn a similar metric on its own.\n",
        "\n",
        "2. Using Spark to enforce an equal distribution of examples over fixed\n",
        "\t bucket sizes. While the `QuantileDiscretizer` does produce an equal\n",
        "\t number of labels per class, the bucket sizes become unacceptably\n",
        "\t close near zero percent change. The most promising resolution to\n",
        "\t unequal class distributions is to apply `Bucketizer` over fixed\n",
        "\t buckets and drop examples from highly represented classes.\n",
        "\n",
        "3. Implementing metrics that are more indicative of the model's\n",
        "\t performance. While categorical and top k categorical accuracy can\n",
        "\t provide some idea of the model's theoretical performance, these\n",
        "\t metrics are proxies for the most relevant metric: profitability.\n",
        "\t The model's performance in a real trading environment will be\n",
        "\t determined primarily by how well high-confidence predictions match\n",
        "\t future trends, as these predictions will form the basis for trading\n",
        "\t decisions. Future work could incorporate this model\n",
        "\t into a reinforcement learning architecture, where network\n",
        "\t performance can be directly assessed based on its profitability.\n",
        "\n",
        "4. Expanding the one dimensional convolution input to two dimensions\n",
        "\t by including the top $n$ correlated stocks over each training\n",
        "\t example window. This would allow the network to condition its\n",
        "\t output on the price trends of correlated\n",
        "\t stocks in addition to the stock that has been targeted for\n",
        "\t prediction.\n",
        "\n",
        "4. Conduct automated experiments on the best choice of network\n",
        "\t architecture in terms of repeat count for each convolutional encoder\n",
        "\t layer. The selection of layer repeats used for experimentation was\n",
        "\t inspired by Resnet-50, but other parameterizations may lead to\n",
        "\t improved performance.\n",
        "\n",
        "## References\n",
        "\n",
        "* [This article on optimizer tuning](https://medium.com/octavian-ai/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2)\n",
        "* [Tensorflow's neural machine translation\n",
        "\tdemo](https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention),\n",
        "\twhich inspired the multi-head attention layer implementation.\n",
        "\n",
        "* [Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "* [Deep Residual Learning for Image\n",
        "\tRecognition](https://arxiv.org/abs/1512.03385), which inspired the\n",
        "\tconvolutional encoder design.\n"
      ]
    }
  ]
}