{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from trader.model import TraderNet\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ProgbarLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=256\n",
    "VALIDATION_SIZE=1000\n",
    "TRAIN_LIMIT=None\n",
    "\n",
    "levels = [3, 3, 5, 2]\n",
    "\n",
    "TFRECORD_DIR = '/mnt/data/dest/tfrecords'\n",
    "SRC_DIR = '/mnt/data/src'\n",
    "ARTIFACTS_DIR = '/mnt/artifacts'\n",
    "\n",
    "# Hide xorg GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "feature_keys = ['close', 'volume', 'position']\n",
    "#feature_keys = ['high', 'low', 'open', 'close', 'volume', 'position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def standardize(*t, axis=0):\n",
    "    features, label= t\n",
    "\n",
    "    mean, var = tf.nn.moments(features, axes=axis, keepdims=True)\n",
    "    length = tf.shape(features, out_type=tf.int32)[axis]\n",
    "    length = tf.cast(length, tf.float32)\n",
    "\n",
    "\n",
    "    std = tf.math.maximum(tf.math.sqrt(var), 1.0 / tf.math.sqrt(length))\n",
    "    n = (features - mean) / std\n",
    "\n",
    "    return tf.cast(n, tf.float16), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecords():\n",
    "\n",
    "    filenames = [os.path.join(TFRECORD_DIR, x) for x in os.listdir(TFRECORD_DIR) if 'part-r-' in x and not x[0] == '.']\n",
    "\n",
    "    feature_col = tf.io.FixedLenSequenceFeature([], tf.float32, default_value=0.0, allow_missing=True)\n",
    "\n",
    "    # Create a description of the features.\n",
    "    feature_description = {\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64, default_value=2),\n",
    "        'change': tf.io.FixedLenFeature([], tf.float32, default_value=2),\n",
    "        'weight': tf.io.FixedLenFeature([], tf.float32, default_value=2),\n",
    "        'high': feature_col,\n",
    "        'low': feature_col,\n",
    "        'open': feature_col,\n",
    "        'close': feature_col,\n",
    "        'volume': feature_col,\n",
    "        'position': feature_col,\n",
    "    }\n",
    "\n",
    "    def _parse_function(example_proto):\n",
    "        return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    def _fold_function(x):\n",
    "        label = tf.cast(x['label'], tf.uint8)\n",
    "        #weight = tf.cast(x['weight'], tf.float16)\n",
    "\n",
    "        feature_list = [tf.math.l2_normalize(x[k]) for k in feature_keys]\n",
    "\n",
    "        features = tf.stack(feature_list)\n",
    "        features = tf.transpose(features)\n",
    "\n",
    "        return (features, label)\n",
    "\n",
    "    ds = tf.data.TFRecordDataset(filenames)\n",
    "    ds = ds.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.map(_fold_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    print(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_callback():\n",
    "\n",
    "    _ = datetime.now()\n",
    "    date_prefix = _.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    checkpoint_dir = os.path.join(ARTIFACTS_DIR, 'checkpoint', date_prefix)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    chkpt_fmt=os.path.join(checkpoint_dir, 'trader_{epoch:02d}.hdf5')\n",
    "\n",
    "    max_checkpoint=5\n",
    "    result = ModelCheckpoint(\n",
    "            filepath=chkpt_fmt,\n",
    "            save_freq='epoch',\n",
    "            save_weights_only=True\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_tensorboard_callback():\n",
    "    _ = datetime.now()\n",
    "    date_prefix = _.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    log_dir=os.path.join(ARTIFACTS_DIR, 'tblogs', date_prefix)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    result = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        write_graph=True,\n",
    "        histogram_freq=1,\n",
    "        embeddings_freq=1,\n",
    "        update_freq=2000\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_lr_callback():\n",
    "    mon = 'loss'\n",
    "    result = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor=mon,\n",
    "                factor=0.2,\n",
    "                patience=5,\n",
    "                min_lr=0.001\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def get_stopping_callback():\n",
    "    return tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def preprocess(ds):\n",
    "\n",
    "    #ds = ds.map(standardize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.shuffle(1024)\n",
    "\n",
    "    train = ds.skip(VALIDATION_SIZE)\n",
    "    validate = ds.take(VALIDATION_SIZE)\n",
    "\n",
    "    validate_batch = VALIDATION_SIZE // BATCH_SIZE\n",
    "    validate = validate.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    if TRAIN_LIMIT:\n",
    "        train = train.take(TRAIN_LIMIT).repeat()\n",
    "        validate = validate.repeat()\n",
    "\n",
    "    train = train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    train = train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    validate = validate.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "    return train, validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    validation_steps = VALIDATION_SIZE // BATCH_SIZE\n",
    "\n",
    "\n",
    "    ds = read_tfrecords()\n",
    "    train, validate = preprocess(ds)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(\n",
    "        shape=[180, len(feature_keys)],\n",
    "        name='input',\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "\n",
    "    if not TRAIN_LIMIT:\n",
    "        epoch_steps = None\n",
    "    else:\n",
    "        epoch_steps = TRAIN_LIMIT // BATCH_SIZE\n",
    "\n",
    "    # Model\n",
    "    model = TraderNet(levels=levels, use_head=True, use_tail=True)\n",
    "    outputs = model(inputs)\n",
    "    model.summary()\n",
    "\n",
    "    # Metrics / loss / optimizer\n",
    "    metrics = [\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=2),\n",
    "    ]\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    callbacks = [\n",
    "            get_checkpoint_callback(),\n",
    "            get_tensorboard_callback(),\n",
    "            get_lr_callback(),\n",
    "            get_stopping_callback()\n",
    "    ]\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    history = model.fit(\n",
    "            train,\n",
    "            epochs=32,\n",
    "            steps_per_epoch=epoch_steps,\n",
    "            validation_data=validate,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
